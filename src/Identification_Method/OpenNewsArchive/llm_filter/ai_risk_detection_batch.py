import os
import time
import json
import logging
import asyncio
import random
from pathlib import Path
from bs4 import BeautifulSoup
import numpy as np
from callmodel import call_model
import re
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import configparser

current_dir = os.path.dirname(os.path.abspath(__file__))
config_path = os.path.join(current_dir, "../../../../config/Identification_Method-OpenNewsArchive-llm_filter-config.ini")
# åˆå§‹åŒ–é…ç½®
config = configparser.ConfigParser()

config.read(config_path)
ONA_config = config["OpenNewsArchive"]
BASE_INPUT_DIR = os.path.join(current_dir,ONA_config.get("BASE_INPUT_DIR"))
BASE_OUTPUT_DIR = os.path.join(current_dir,ONA_config.get("BASE_OUTPUT_DIR"))  # è¾“å‡ºç›®å½•
LOG_FILE = os.path.join(current_dir,ONA_config.get("LOG_FILE"))
#BATCH_SIZE = 10
CONTENT_LIMIT = ONA_config.getint("CONTENT_LIMIT", 10000)  # é»˜è®¤å†…å®¹é™åˆ¶ä¸º10000å­—ç¬¦

# === å…¨å±€æ¨ç†è®¡æ•°å™¨ ===
global_inference_counter = 0

prompt_path= os.path.join(current_dir, "../../../../prompt/Identification_Method-OpenNewsArchive_Datasets-llm_filter-prompt.md")
with open(prompt_path, 'r', encoding='utf-8') as f:
    md_content = f.read()
# æç¤ºè¯
PROMPT = str(md_content)


async def detect_ai_risk_batches(text_batches, system_prompt=PROMPT):
    def fetch_model_response(texts):
        global global_inference_counter
        try:
            formatted_input = texts  # è¿™é‡Œæ˜¯ JSON æ ¼å¼çš„æ–‡æœ¬
            response,tokens= call_model(system_prompt, formatted_input)
            # è®¡æ•°æ¨ç†æ¬¡æ•°
            global_inference_counter += 1
            if global_inference_counter % 1000 == 0:
                logger.info(f"ğŸš¦ å·²æ¨ç† {global_inference_counter} æ¬¡ï¼Œä¼‘çœ  120 ç§’ä»¥é˜²æ­¢å µå¡...")
                time.sleep(120)  # ç”¨ time.sleep è¿™é‡Œå°±å¤Ÿäº†ï¼Œå› ä¸ºæ˜¯åœ¨åŒæ­¥éƒ¨åˆ†
            return response,tokens
        except Exception as e:
            logger.error(f"ğŸ›‘ æœ¬åœ°æ¨¡å‹è¯·æ±‚å¤±è´¥: {e}")
            return (["AIGCrisk_Irrelevant"] * len(texts), 0)

    async def fetch(texts):
        for attempt in range(3):
            try:
                start=time.time()
                response_str,tokens= fetch_model_response(texts)
                end=time.time()
                logger.info(f"[Model] æ‰¹æ¬¡æ¨ç†å®Œæˆ,ç”¨æ—¶{end-start:.2f}ç§’")
                #print(f"response_str: 1{response_str}2")
                if not response_str:
                    logger.warning(f"âš ï¸ å“åº”ä¸ºç©ºæˆ–ç¼ºå°‘ 'response' å­—æ®µ:")
                    return (["AIGCrisk_Irrelevant"] * len(texts), 0)

                if isinstance(response_str, str):
                    try:
                        start_idx = response_str.find('[')
                        end_idx = response_str.rfind(']')
                        if start_idx != -1 and end_idx != -1:
                            list_str = response_str[start_idx:end_idx + 1]
                            if list_str.startswith('"') and list_str.endswith('"'):
                                list_str = list_str[1:-1]
                            response_list = json.loads(list_str)
                            return response_list,tokens
                        else:
                            logger.warning(f"âš ï¸ å“åº”ä¸­ç¼ºå°‘ä¸­æ‹¬å·: {response_str[:]}")
                    except Exception as e:
                        logger.warning(f"âš ï¸ å“åº”è§£æå¤±è´¥: {e}\nåŸå§‹ response å­—ç¬¦ä¸²: {response_str[:]}")
            except TimeoutError as e:
                logger.error(f"â±ï¸ API è¯·æ±‚è¶…æ—¶ï¼ˆç¬¬ {attempt+1} æ¬¡ï¼‰: {e}")
            except Exception as e:
                logger.error(f"ğŸ›‘ æœªçŸ¥é”™è¯¯ï¼ˆç¬¬ {attempt+1} æ¬¡ï¼‰: {type(e).__name__}: {e}")
            await asyncio.sleep(1)
        return (["AIGCrisk_Irrelevant"] * len(texts), 0)
    tasks = [fetch(batch) for batch in text_batches]
    return await asyncio.gather(*tasks)
# å°†è¾“å…¥æ ¼å¼è½¬æ¢ä¸º JSONï¼Œç»“æ„åŒ–çš„å‘é€ç»™æ¨¡å‹
def prepare_input_for_model(batch_data):
    # æ ¼å¼åŒ–è¾“å…¥ä¸º JSONï¼Œæ¯ä¸ªæ–°é—»ä¸ºä¸€ä¸ªå¯¹è±¡ï¼ŒåŒ…æ‹¬æ ‡é¢˜å’Œæ­£æ–‡
    input_data = [{"title": title, "content": content} for _, title, content in batch_data]
    return json.dumps(input_data, ensure_ascii=False)

async def handle_batch(batch_data, output_dir):
    #æ‰¹æ¬¡æ¨ç†ä¸æ–‡ä»¶å¤„ç†
    texts = prepare_input_for_model(batch_data)
    results_and_tokens = await detect_ai_risk_batches([texts])
    relevant_count = 0
    results, total_tokens = results_and_tokens[0]
    for (fpath, title, original_txt), result in zip(batch_data, results):
        logger.info(f"æ–‡ä»¶å·²å¤„ç†: {fpath.stem}.txt | ç»“æœ: {result}")
    # è®°å½•æœ¬æ‰¹æ¬¡çš„ tokens
    logger.info(f"æœ¬æ‰¹æ¬¡ä½¿ç”¨tokens: {total_tokens}")
    for (fpath, title, original_txt), result in zip(batch_data, results):
        if result == 'AIGCrisk_relevant':
            try:
                combined = f"{title}\n{original_txt}"[:CONTENT_LIMIT]
                out_path = output_dir / f"{fpath.stem}.txt"
                if out_path.exists():
                    logger.warning(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ‹’ç»è¦†ç›–: {out_path}")
                    continue
                else:
                    out_path.write_text(combined, encoding="utf-8", errors="ignore")

                logger.info(f"âœ… ä¿å­˜é£é™©æ–‡æœ¬: {fpath.stem}.txt ")
                relevant_count += 1
            except Exception as e:
                logger.error(f"âŒ ä¿å­˜å¤±è´¥: {e}")
    return relevant_count,total_tokens

async def main():
    start = time.time()
    total_files = 0
    relevant_files = 0
    total_tokens_per_batch = {3: []}  # ç”¨äºå­˜å‚¨ä¸åŒæ‰¹æ¬¡çš„tokensæ•°é‡

    ym_dirs=[Path(BASE_INPUT_DIR)]#ä¸æœˆä»½æ— å…³
    for ym_dir in ym_dirs:
        txt_files = list(ym_dir.glob("*.txt"))
        if not txt_files:
            
            continue

        #html_files = random.sample(html_files, min(len(html_files), 1500))
        
        logger.info(f"å¤„ç†ç›®å½•: {ym_dir} éšæœºæŠ½å– {len(txt_files)} ä¸ªæ–‡ä»¶")

        rel_path = ym_dir.relative_to(BASE_INPUT_DIR)
        print(f"rel_path: {rel_path}")
        output_dir = Path(BASE_OUTPUT_DIR) / rel_path
        print(f"output_dir: {output_dir}")
        output_dir.mkdir(parents=True, exist_ok=True)

        batch_data = []
        for batch_size in [3]:  # ä¾æ¬¡æµ‹è¯•æ‰¹æ¬¡å¤§å°ä¸º5, 10, 15çš„æƒ…å†µ
            logger.info(f"å¼€å§‹å¤„ç†æ‰¹æ¬¡å¤§å°: {batch_size}")
            for txt_path in txt_files:
                try:
                    with open(txt_path, 'r', encoding='utf-8') as f:
                        title = f.readline().strip()
                        content = f.read().strip()


                    batch_data.append((txt_path, title, content))
                    logger.info(f"æ·»åŠ åˆ°æ‰¹æ¬¡: {txt_path.name} | æ‰¹æ¬¡å¤§å°: {len(batch_data)}")
                    if len(batch_data) >= batch_size:
                        relevant_count, batch_tokens = await handle_batch(batch_data, output_dir)
                        relevant_files += relevant_count
                        total_files += len(batch_data)
                        total_tokens_per_batch[batch_size].append(batch_tokens)  # è®°å½•æ¯ä¸ªæ‰¹æ¬¡çš„tokensæ•°é‡
                        batch_data.clear()  # æ¸…ç©ºbatch_dataï¼Œå‡†å¤‡ä¸‹ä¸€ä¸ªæ‰¹æ¬¡

                except Exception as e:
                    logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {html_path}: {e}")

            # å¤„ç†å‰©ä½™çš„æ–‡ä»¶
            if batch_data:
                relevant_count, batch_tokens = await handle_batch(batch_data, output_dir)
                relevant_files += relevant_count
                total_files += len(batch_data)
                total_tokens_per_batch[batch_size].append(batch_tokens)  
    # è®¡ç®—æ¯ç§æ‰¹æ¬¡å¤§å°çš„å¹³å‡tokens
    for batch_size, tokens_list in total_tokens_per_batch.items():
        avg_tokens = np.mean(tokens_list) if tokens_list else 0
        logger.info(f"æ‰¹æ¬¡å¤§å° {batch_size}: å¹³å‡ä½¿ç”¨ {avg_tokens:.2f} tokens")
    logger.info(f"ğŸ•’ æ€»è€—æ—¶ï¼š{time.time() - start:.2f}s")
    if total_files > 0:
        rate = 100 * relevant_files / total_files
        logger.info(f"ğŸ“Š æ€»å…±å¤„ç† {total_files} ç¯‡æ–°é—»ï¼Œé£é™©ç›¸å…³ {relevant_files} ç¯‡ï¼Œæ¯”ä¾‹ {rate:.2f}%")
    else:
        logger.info("ğŸ“­ æœªå¤„ç†ä»»ä½•æ–‡ä»¶")

if __name__ == "__main__":

    asyncio.run(main())
